{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31154,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!touch req.txt","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-11-03T14:25:33.962010Z","iopub.execute_input":"2025-11-03T14:25:33.962471Z","iopub.status.idle":"2025-11-03T14:25:34.083261Z","shell.execute_reply.started":"2025-11-03T14:25:33.962446Z","shell.execute_reply":"2025-11-03T14:25:34.082400Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"!pip install easydict\n!pip install addict\n!pip install einops\n!pip install tokenizers==0.20.3\n\n!pip install transformers==4.46.3\n\n# !pip install\n# !pip install","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-03T14:26:43.864922Z","iopub.execute_input":"2025-11-03T14:26:43.865178Z","iopub.status.idle":"2025-11-03T14:27:11.520436Z","shell.execute_reply.started":"2025-11-03T14:26:43.865157Z","shell.execute_reply":"2025-11-03T14:27:11.519657Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: easydict in /usr/local/lib/python3.11/dist-packages (1.13)\nCollecting addict\n  Downloading addict-2.4.0-py3-none-any.whl.metadata (1.0 kB)\nDownloading addict-2.4.0-py3-none-any.whl (3.8 kB)\nInstalling collected packages: addict\nSuccessfully installed addict-2.4.0\nRequirement already satisfied: einops in /usr/local/lib/python3.11/dist-packages (0.8.1)\nCollecting tokenizers==0.20.3\n  Downloading tokenizers-0.20.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\nCollecting huggingface-hub<1.0,>=0.16.4 (from tokenizers==0.20.3)\n  Downloading huggingface_hub-0.36.0-py3-none-any.whl.metadata (14 kB)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers==0.20.3) (3.19.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers==0.20.3) (2025.9.0)\nRequirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers==0.20.3) (25.0)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers==0.20.3) (6.0.3)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers==0.20.3) (2.32.5)\nRequirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers==0.20.3) (4.67.1)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers==0.20.3) (4.15.0)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers==0.20.3) (1.1.10)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers==0.20.3) (3.4.3)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers==0.20.3) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers==0.20.3) (2.5.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers==0.20.3) (2025.8.3)\nDownloading tokenizers-0.20.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m34.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading huggingface_hub-0.36.0-py3-none-any.whl (566 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m566.1/566.1 kB\u001b[0m \u001b[31m34.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: huggingface-hub, tokenizers\n  Attempting uninstall: huggingface-hub\n    Found existing installation: huggingface-hub 1.0.0rc2\n    Uninstalling huggingface-hub-1.0.0rc2:\n      Successfully uninstalled huggingface-hub-1.0.0rc2\n  Attempting uninstall: tokenizers\n    Found existing installation: tokenizers 0.21.2\n    Uninstalling tokenizers-0.21.2:\n      Successfully uninstalled tokenizers-0.21.2\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ndatasets 4.1.1 requires pyarrow>=21.0.0, but you have pyarrow 19.0.1 which is incompatible.\ngradio 5.38.1 requires pydantic<2.12,>=2.0, but you have pydantic 2.12.0a1 which is incompatible.\ntransformers 4.53.3 requires tokenizers<0.22,>=0.21, but you have tokenizers 0.20.3 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed huggingface-hub-0.36.0 tokenizers-0.20.3\nCollecting transformers==4.46.3\n  Downloading transformers-4.46.3-py3-none-any.whl.metadata (44 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.1/44.1 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers==4.46.3) (3.19.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.11/dist-packages (from transformers==4.46.3) (0.36.0)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.46.3) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers==4.46.3) (25.0)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers==4.46.3) (6.0.3)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.46.3) (2025.9.18)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers==4.46.3) (2.32.5)\nRequirement already satisfied: tokenizers<0.21,>=0.20 in /usr/local/lib/python3.11/dist-packages (from transformers==4.46.3) (0.20.3)\nRequirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers==4.46.3) (0.5.3)\nRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers==4.46.3) (4.67.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers==4.46.3) (2025.9.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers==4.46.3) (4.15.0)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers==4.46.3) (1.1.10)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers==4.46.3) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers==4.46.3) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers==4.46.3) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers==4.46.3) (2025.2.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers==4.46.3) (2022.2.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers==4.46.3) (2.4.1)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.46.3) (3.4.3)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.46.3) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.46.3) (2.5.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.46.3) (2025.8.3)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers==4.46.3) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers==4.46.3) (2022.2.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->transformers==4.46.3) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->transformers==4.46.3) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->transformers==4.46.3) (2024.2.0)\nDownloading transformers-4.46.3-py3-none-any.whl (10.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.0/10.0 MB\u001b[0m \u001b[31m77.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m:01\u001b[0m\n\u001b[?25hInstalling collected packages: transformers\n  Attempting uninstall: transformers\n    Found existing installation: transformers 4.53.3\n    Uninstalling transformers-4.53.3:\n      Successfully uninstalled transformers-4.53.3\nSuccessfully installed transformers-4.46.3\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"from transformers import AutoModel, AutoTokenizer\nimport torch\nimport os\nmodel_name = 'deepseek-ai/DeepSeek-OCR'\n\ntokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\nmodel = AutoModel.from_pretrained(model_name, trust_remote_code=True, use_safetensors=True)\nmodel = model.eval().cuda().to(torch.bfloat16)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-03T14:27:26.495862Z","iopub.execute_input":"2025-11-03T14:27:26.496475Z","iopub.status.idle":"2025-11-03T14:29:05.612001Z","shell.execute_reply.started":"2025-11-03T14:27:26.496446Z","shell.execute_reply":"2025-11-03T14:29:05.611090Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0cb3aefb65b640cdab76cf8ea0ccab1f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9e8ced6c29ec4431afc1026acc8eb3e8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/801 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"eaf3861ce84e4783954cdd7c8285ce41"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ae0877261f8947d391a9ae9a73ecc5a9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"modeling_deepseekocr.py: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cf77181ba3e942c48105a0c14c718e20"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"deepencoder.py: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e0fd535635394036b6fc86d6b77e683d"}},"metadata":{}},{"name":"stderr","text":"A new version of the following files was downloaded from https://huggingface.co/deepseek-ai/DeepSeek-OCR:\n- deepencoder.py\n. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"configuration_deepseek_v2.py: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c0db9bc6090646eda7a7f6c39b1c2fd8"}},"metadata":{}},{"name":"stderr","text":"A new version of the following files was downloaded from https://huggingface.co/deepseek-ai/DeepSeek-OCR:\n- configuration_deepseek_v2.py\n. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"conversation.py: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0af0cf6c659f45ee80fec72c1a2d875b"}},"metadata":{}},{"name":"stderr","text":"A new version of the following files was downloaded from https://huggingface.co/deepseek-ai/DeepSeek-OCR:\n- conversation.py\n. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"modeling_deepseekv2.py: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7cafbf82ca104d31ad284c26eddd8218"}},"metadata":{}},{"name":"stderr","text":"A new version of the following files was downloaded from https://huggingface.co/deepseek-ai/DeepSeek-OCR:\n- modeling_deepseekv2.py\n. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\nA new version of the following files was downloaded from https://huggingface.co/deepseek-ai/DeepSeek-OCR:\n- modeling_deepseekocr.py\n- deepencoder.py\n- configuration_deepseek_v2.py\n- conversation.py\n- modeling_deepseekv2.py\n. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n2025-11-03 14:27:38.905827: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1762180059.135421      37 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1762180059.193176      37 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nYou are using a model of type deepseek_vl_v2 to instantiate a model of type DeepseekOCR. This is not supported for all configurations of models and can yield errors.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ae44a2eb0fc14ae8b9938b3a0d9ec118"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading shards:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ba3993e1c72345bf901192cfa04d79a4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-000001.safetensors:   0%|          | 0.00/6.67G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"489d62b214b642059ed6d3baf85e3399"}},"metadata":{}},{"name":"stderr","text":"Some weights of DeepseekOCRForCausalLM were not initialized from the model checkpoint at deepseek-ai/DeepSeek-OCR and are newly initialized: ['model.vision_model.embeddings.position_ids']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"from transformers import TextStreamer\n\nclass NoEOSTextStreamer(TextStreamer):\n    def on_finalized_text(self, text: str, stream_end: bool = False):\n\n        eos_text = self.tokenizer.decode([self.tokenizer.eos_token_id], skip_special_tokens=False)\n        text = text.replace(eos_text, \"\\n\")\n        print(text, flush=True, end=\"\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-03T14:47:14.658915Z","iopub.execute_input":"2025-11-03T14:47:14.659196Z","iopub.status.idle":"2025-11-03T14:47:14.664553Z","shell.execute_reply.started":"2025-11-03T14:47:14.659175Z","shell.execute_reply":"2025-11-03T14:47:14.663602Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"\"\"\"\nFrom https://github.com/lm-sys/FastChat/blob/main/fastchat/conversation.py\n\"\"\"\n\nimport dataclasses\nfrom enum import IntEnum, auto\nfrom typing import Any, Dict, List\n\n\nclass SeparatorStyle(IntEnum):\n    \"\"\"Separator styles.\"\"\"\n\n    DeepSeek = auto()\n    DeepSeekV2 = auto()\n    PLAIN = auto()\n    ALIGNMENT = auto()\n\n\n@dataclasses.dataclass\nclass Conversation:\n    \"\"\"A class that manages prompt templates and keeps all conversation history.\"\"\"\n\n    # The name of this template\n    name: str\n    # The template of the system prompt\n    system_template: str = \"{system_message}\"\n    # The system message\n    system_message: str = \"\"\n    # The names of two roles\n    roles: List[str] = ((\"USER\", \"ASSISTANT\"),)\n    # All messages. Each item is (role, message).\n    messages: List[List[str]] = ()\n    # The number of few shot examples\n    offset: int = 0\n    # The separator style and configurations\n    sep_style: SeparatorStyle = SeparatorStyle.DeepSeek\n    sep: str = \"\\n\"\n    sep2: str = None\n    # Stop criteria (the default one is EOS token)\n    stop_str: str = None\n    # Stops generation if meeting any token in this list\n    stop_token_ids: List[int] = None\n\n    def get_prompt(self) -> str:\n        \"\"\"Get the prompt for generation.\"\"\"\n        system_prompt = self.system_template.format(system_message=self.system_message)\n        if self.sep_style == SeparatorStyle.DeepSeek:\n            seps = [self.sep, self.sep2]\n            if system_prompt == \"\" or system_prompt is None:\n                ret = \"\"\n            else:\n                ret = system_prompt + seps[0]\n            for i, (role, message) in enumerate(self.messages):\n                if message:\n                    ret += role + \": \" + message + seps[i % 2]\n                else:\n                    ret += role + \":\"\n            return ret\n        elif self.sep_style == SeparatorStyle.DeepSeekV2:\n            seps = [self.sep, self.sep2]\n            if system_prompt == \"\" or system_prompt is None:\n                ret = \"\"\n            else:\n                ret = system_prompt + seps[0]\n            for i, (role, message) in enumerate(self.messages):\n                if message:\n                    if role == \"User\":\n                        ret += \"<｜sft▁begin｜>\\n\" + message + self.sep #<｜sft▁begin｜>User Input<｜sft▁end｜>\\nResponse<｜end▁of▁sentence｜>\n                    else:\n                        ret += message + self.sep2\n                else:\n                    ret = ret\n            return ret\n\n        elif self.sep_style == SeparatorStyle.PLAIN:\n            seps = [self.sep, self.sep2]\n            ret = \"\"\n            for i, (role, message) in enumerate(self.messages):\n                if message:\n                    if type(message) is tuple:\n                        message, _, _ = message\n                    if i % 2 == 0:\n                        ret += message + seps[i % 2]\n                    else:\n                        ret += message + seps[i % 2]\n                else:\n                    ret += \"\"\n            return ret\n        elif self.sep_style == SeparatorStyle.ALIGNMENT:\n            seps = [self.sep, self.sep2]\n            ret = \"\"\n            for i, (role, message) in enumerate(self.messages):\n                if message:\n                    if type(message) is tuple:\n                        message, _, _ = message\n                    if i % 2 == 0:\n                        ret += '<image>\\n' + seps[i % 2]\n                    else:\n                        ret += message + seps[i % 2]\n                else:\n                    ret += \"\"\n            return ret\n        else:\n            raise ValueError(f\"Invalid style: {self.sep_style}\")\n\n    def set_system_message(self, system_message: str):\n        \"\"\"Set the system message.\"\"\"\n        self.system_message = system_message\n\n    def append_message(self, role: str, message: str):\n        \"\"\"Append a new message.\"\"\"\n        self.messages.append([role, message])\n\n    def update_last_message(self, message: str):\n        \"\"\"Update the last output.\n\n        The last message is typically set to be None when constructing the prompt,\n        so we need to update it in-place after getting the response from a model.\n        \"\"\"\n        self.messages[-1][1] = message\n\n    def reset_message(self):\n        \"\"\"Reset a new message.\"\"\"\n        self.messages = []\n\n    def to_gradio_chatbot(self):\n        \"\"\"Convert the conversation to gradio chatbot format.\"\"\"\n        ret = []\n        for i, (role, msg) in enumerate(self.messages[self.offset :]):\n            if i % 2 == 0:\n                ret.append([msg, None])\n            else:\n                ret[-1][-1] = msg\n        return ret\n\n    def to_openai_api_messages(self):\n        \"\"\"Convert the conversation to OpenAI chat completion format.\"\"\"\n        system_prompt = self.system_template.format(system_message=self.system_message)\n        ret = [{\"role\": \"system\", \"content\": system_prompt}]\n\n        for i, (_, msg) in enumerate(self.messages[self.offset :]):\n            if i % 2 == 0:\n                ret.append({\"role\": \"user\", \"content\": msg})\n            else:\n                if msg is not None:\n                    ret.append({\"role\": \"assistant\", \"content\": msg})\n        return ret\n\n    def copy(self):\n        return Conversation(\n            name=self.name,\n            system_template=self.system_template,\n            system_message=self.system_message,\n            roles=self.roles,\n            messages=[[x, y] for x, y in self.messages],\n            offset=self.offset,\n            sep_style=self.sep_style,\n            sep=self.sep,\n            sep2=self.sep2,\n            stop_str=self.stop_str,\n            stop_token_ids=self.stop_token_ids,\n        )\n\n    def dict(self):\n        return {\n            \"template_name\": self.name,\n            \"system_message\": self.system_message,\n            \"roles\": self.roles,\n            \"messages\": self.messages,\n            \"offset\": self.offset,\n        }\n\n\n# A global registry for all conversation templates\nconv_templates: Dict[str, Conversation] = {}\n\n\ndef register_conv_template(template: Conversation, override: bool = False):\n    \"\"\"Register a new conversation template.\"\"\"\n    if not override:\n        assert template.name not in conv_templates, f\"{template.name} has been registered.\"\n\n    conv_templates[template.name] = template\n\n\ndef get_conv_template(name: str) -> Conversation:\n    \"\"\"Get a conversation template.\"\"\"\n    return conv_templates[name].copy()\n\n\nregister_conv_template(\n    Conversation(\n        name=\"deepseek\",\n        system_template=\"{system_message}\",\n        # system_message=\"You are a helpful assistant. Please answer truthfully and write out your \"\n        # \"thinking step by step to be sure you get the right answer.\",\n        system_message=\"\",\n        roles=(\"<|User|>\", \"<|Assistant|>\"),\n        messages=(),\n        offset=0,\n        sep_style=SeparatorStyle.DeepSeek,\n        sep=\"\\n\\n\",\n        sep2=\"<｜end▁of▁sentence｜>\",\n        stop_token_ids=[100001],\n        stop_str=[\"User:\", \"<｜end▁of▁sentence｜>\"]\n    )\n)\nregister_conv_template(\n    Conversation(\n        name=\"deepseekv2\",\n        system_template=\"{system_message}\",\n        # system_message=\"You are a helpful assistant. Please answer truthfully and write out your \"\n        # \"thinking step by step to be sure you get the right answer.\",\n        system_message=\"\",\n        roles=(\"<｜User｜>\", \"<｜Assistant｜>\"),\n        messages=(),\n        offset=0,\n        sep_style=SeparatorStyle.DeepSeek,\n        sep=\"\",\n        sep2=\"<｜end▁of▁sentence｜>\",\n        stop_token_ids=[100001],\n        stop_str=[\"User:\", \"<｜end▁of▁sentence｜>\"]\n    )\n)\n\n\nregister_conv_template(\n    Conversation(\n        name=\"plain\",\n        system_template=\"\",\n        system_message=\"\",\n        roles=(\"\", \"\"),\n        messages=(),\n        offset=0,\n        sep_style=SeparatorStyle.PLAIN,\n        sep=\"\",\n        sep2=\"\",\n        stop_token_ids=[100001],\n        stop_str=['</s>'],\n    )\n)\n\n\nregister_conv_template(\n    Conversation(\n        name=\"alignment\",\n        system_template=\"\",\n        system_message=\"\",\n        roles=(\"\", \"\"),\n        messages=(),\n        offset=0,\n        sep_style=SeparatorStyle.ALIGNMENT,\n        sep=\"\",\n        sep2=\"\",\n        stop_token_ids=[100001],\n        stop_str=['</s>'],\n    )\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-03T14:49:06.762211Z","iopub.execute_input":"2025-11-03T14:49:06.763006Z","iopub.status.idle":"2025-11-03T14:49:06.784091Z","shell.execute_reply.started":"2025-11-03T14:49:06.762964Z","shell.execute_reply":"2025-11-03T14:49:06.783250Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"def normalize_transform(mean, std):\n    if mean is None and std is None:\n        transform = None\n    elif mean is None and std is not None:\n        mean = [0.] * len(std)\n        transform = transforms.Normalize(mean=mean, std=std)\n    elif mean is not None and std is None:\n        std = [1.] * len(mean)\n        transform = transforms.Normalize(mean=mean, std=std)\n    else:\n        transform = transforms.Normalize(mean=mean, std=std)\n\n    return transform","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-03T14:56:13.673966Z","iopub.execute_input":"2025-11-03T14:56:13.674238Z","iopub.status.idle":"2025-11-03T14:56:13.679118Z","shell.execute_reply.started":"2025-11-03T14:56:13.674220Z","shell.execute_reply":"2025-11-03T14:56:13.678361Z"}},"outputs":[],"execution_count":29},{"cell_type":"code","source":"from typing import List, Optional, Tuple, Union\nclass BasicImageTransform:\n    def __init__(\n        self, \n        mean: Optional[Tuple[float, float, float]] = (0.5, 0.5, 0.5),\n        std: Optional[Tuple[float, float, float]] = (0.5, 0.5, 0.5),\n        normalize: bool = True\n    ):\n        self.mean = mean\n        self.std = std\n    \n        transform_pipelines = [\n            transforms.ToTensor()\n        ]\n\n        normalize = normalize_transform(mean, std) if normalize else nn.Identity()\n        if normalize is not None:\n            transform_pipelines.append(normalize)\n\n        self.transform = transforms.Compose(transform_pipelines)\n    \n    def __call__(self, x):\n        x = self.transform(x)\n        return x","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-03T14:50:16.184071Z","iopub.execute_input":"2025-11-03T14:50:16.184646Z","iopub.status.idle":"2025-11-03T14:50:16.190066Z","shell.execute_reply.started":"2025-11-03T14:50:16.184623Z","shell.execute_reply":"2025-11-03T14:50:16.189358Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"import os\nimport math\nimport torch\nfrom PIL import Image, ImageOps\nfrom typing import List, Dict\nfrom transformers import AutoTokenizer\nfrom transformers import AutoModelForCausalLM\nfrom tqdm import tqdm\n\ndef draw_bounding_boxes(image, refs, ouput_path):\n\n    image_width, image_height = image.size\n    \n    img_draw = image.copy()\n    draw = ImageDraw.Draw(img_draw)\n\n    overlay = Image.new('RGBA', img_draw.size, (0, 0, 0, 0))\n    draw2 = ImageDraw.Draw(overlay)\n    \n    # try:\n    # except IOError:\n    #     try:\n    #         font = ImageFont.truetype(\"DejaVuSans.ttf\", 20) \n    #     except IOError:\n    font = ImageFont.load_default()\n\n    img_idx = 0\n    \n    for i, ref in enumerate(refs):\n        try:\n            result = extract_coordinates_and_label(ref, image_width, image_height)\n            if result:\n                label_type, points_list = result\n                \n                color = (np.random.randint(0, 200), np.random.randint(0, 200), np.random.randint(0, 255))\n\n                color_a = color + (20, )\n                for points in points_list:\n                    x1, y1, x2, y2 = points\n\n                    x1 = int(x1 / 999 * image_width)\n                    y1 = int(y1 / 999 * image_height)\n\n                    x2 = int(x2 / 999 * image_width)\n                    y2 = int(y2 / 999 * image_height)\n\n                    if label_type == 'image':\n                        try:\n                            cropped = image.crop((x1, y1, x2, y2))\n                            cropped.save(f\"{ouput_path}/images/{img_idx}.jpg\")\n                        except Exception as e:\n                            print(e)\n                            pass\n                        img_idx += 1\n                        \n                    try:\n                        if label_type == 'title':\n                            draw.rectangle([x1, y1, x2, y2], outline=color, width=4)\n                            draw2.rectangle([x1, y1, x2, y2], fill=color_a, outline=(0, 0, 0, 0), width=1)\n                        else:\n                            draw.rectangle([x1, y1, x2, y2], outline=color, width=2)\n                            draw2.rectangle([x1, y1, x2, y2], fill=color_a, outline=(0, 0, 0, 0), width=1)\n                        text_x = x1\n                        text_y = max(0, y1 - 15)\n                            \n                        \n                        text_bbox = draw.textbbox((0, 0), label_type, font=font)\n                        text_width = text_bbox[2] - text_bbox[0]\n                        text_height = text_bbox[3] - text_bbox[1]\n                        draw.rectangle([text_x, text_y, text_x + text_width, text_y + text_height], \n                                    fill=(255, 255, 255, 30))\n                        \n                        draw.text((text_x, text_y), label_type, font=font, fill=color)\n                    except:\n                        pass\n        except:\n            continue\n    img_draw.paste(overlay, (0, 0), overlay)\n    return img_draw\n\ndef format_messages(\n        conversations: List[Dict[str, str]],\n        sft_format: str = \"deepseek\",\n        system_prompt: str = \"\",\n):\n    \"\"\"\n    Applies the SFT template to conversation.\n    Args:\n        conversations (List[Dict]): A List of messages.\n        sft_format (str, optional): The format of the SFT template to use. Defaults to \"deepseek\".\n        system_prompt (str, optional): The system prompt to use in the SFT template. Defaults to \"\".\n    Returns:\n        sft_prompt (str): The formatted text.\n    \"\"\"\n\n    conv = get_conv_template(sft_format)\n    conv.set_system_message(system_prompt)\n    for message in conversations:\n        conv.append_message(message[\"role\"], message[\"content\"].strip())\n    sft_prompt = conv.get_prompt().strip()\n\n    return sft_prompt\n\ndef find_closest_aspect_ratio(aspect_ratio, target_ratios, width, height, image_size):\n    best_ratio_diff = float('inf')\n    best_ratio = (1, 1)\n    area = width * height\n    for ratio in target_ratios:\n        target_aspect_ratio = ratio[0] / ratio[1]\n        ratio_diff = abs(aspect_ratio - target_aspect_ratio)\n        if ratio_diff < best_ratio_diff:\n            best_ratio_diff = ratio_diff\n            best_ratio = ratio\n        elif ratio_diff == best_ratio_diff:\n            if area > 0.5 * image_size * image_size * ratio[0] * ratio[1]:\n                best_ratio = ratio\n    # print(f'width: {width}, height: {height}, best_ratio: {best_ratio}')\n    return best_ratio\n\ndef dynamic_preprocess(image, min_num=2, max_num=9, image_size=640, use_thumbnail=False):\n    orig_width, orig_height = image.size\n    aspect_ratio = orig_width / orig_height\n\n    # calculate the existing image aspect ratio\n    target_ratios = set(\n        (i, j) for n in range(min_num, max_num + 1) for i in range(1, n + 1) for j in range(1, n + 1) if\n        i * j <= max_num and i * j >= min_num)\n    # print(target_ratios)\n    target_ratios = sorted(target_ratios, key=lambda x: x[0] * x[1])\n\n    # find the closest aspect ratio to the target\n    target_aspect_ratio = find_closest_aspect_ratio(\n        aspect_ratio, target_ratios, orig_width, orig_height, image_size)\n\n    # print(target_aspect_ratio)\n    # calculate the target width and height\n    target_width = image_size * target_aspect_ratio[0]\n    target_height = image_size * target_aspect_ratio[1]\n    blocks = target_aspect_ratio[0] * target_aspect_ratio[1]\n\n    # resize the image\n    resized_img = image.resize((target_width, target_height))\n    processed_images = []\n    for i in range(blocks):\n        box = (\n            (i % (target_width // image_size)) * image_size,\n            (i // (target_width // image_size)) * image_size,\n            ((i % (target_width // image_size)) + 1) * image_size,\n            ((i // (target_width // image_size)) + 1) * image_size\n        )\n        # split the image\n        split_img = resized_img.crop(box)\n        processed_images.append(split_img)\n    assert len(processed_images) == blocks\n    if use_thumbnail and len(processed_images) != 1:\n        thumbnail_img = image.resize((image_size, image_size))\n        processed_images.append(thumbnail_img)\n    return processed_images, target_aspect_ratio\n    \ndef text_encode(tokenizer, text: str, bos: bool = True, eos: bool = False):\n    t = tokenizer.encode(text, add_special_tokens=False)\n    bos_id = 0\n    eos_id = 1\n    if bos:\n        t = [bos_id] + t\n    if eos:\n        t = t + [eos_id]\n\n    return t\n    \ndef infer_with_image_object(\n    model,  # Pass the instantiated model object here\n    tokenizer,\n    image_object: Image.Image,\n    prompt: str = '<image>\\n<|grounding|>Convert the document to markdown. ',\n    output_path: str = '',\n    base_size: int = 1024,\n    image_size: int = 640,\n    crop_mode: bool = True,\n    test_compress: bool = False,\n    save_results: bool = False,\n    eval_mode: bool = True\n):\n    \"\"\"\n    Mimics the behavior of the DeepseekOCRForCausalLM.infer method but\n    operates as an external function and accepts a PIL Image object.\n    \"\"\"\n    \n    # 1. Disable torch init (was self.disable_torch_init())\n    model.disable_torch_init()\n\n    # 2. Setup directories\n    os.makedirs(output_path, exist_ok=True)\n    os.makedirs(f'{output_path}/images', exist_ok=True)\n\n    # 3. Setup conversation and images list\n    if not prompt:\n        assert False, f'prompt is none!'\n    \n    images = []\n    image_draw = None\n    conversation = [\n        {\n            \"role\": \"<|User|>\",\n            # \"content\": \"<image>\\n<|grounding|>Given the layout of the image. \",\n            \"content\": f'{prompt}',\n            # \"content\": \"君不见黄河之水天上来的下一句是什么？\",\n            # \"content\": \"<image>\\nFree OCR. \",\n            # \"content\": \"<image>\\nParse the figure. \",\n            # \"content\": \"<image>\\nExtract the text in the image. \",\n            # \"images\": [f'{image_file}'],\n        },\n        {\"role\": \"<|Assistant|>\", \"content\": \"\"},\n    ]\n\n\n    # 4. Format prompt\n    prompt = format_messages(conversations=conversation, sft_format='plain', system_prompt='')\n    patch_size = 16\n    downsample_ratio = 4\n    images = [image_object]\n\n    # 5. Get image properties (if image exists)\n    valid_img_tokens = 0\n    ratio = 1\n    if image_draw:\n        w, h = image_draw.size\n        ratio = 1 - ((max(w, h) - min(w, h)) / (max(w, h)))\n    \n    # 6. Initialize transforms and tokens\n    image_transform = BasicImageTransform(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5), normalize=True)\n    images_seq_mask = []\n\n    image_token = '<image>'\n    image_token_id = 128815\n    text_splits = prompt.split(image_token)\n\n    images_list, images_crop_list, images_seq_mask = [], [], []\n    tokenized_str = []\n    images_spatial_crop = []\n    for text_sep, image in zip(text_splits, images):\n            tokenized_sep = text_encode(tokenizer, text_sep, bos=False, eos=False)\n\n            tokenized_str += tokenized_sep\n            images_seq_mask += [False] * len(tokenized_sep)\n\n            if crop_mode:\n\n                if image.size[0] <= 640 and image.size[1] <= 640:\n                    crop_ratio = [1, 1]\n\n                else:\n                    if crop_mode:\n                        # best_width, best_height = select_best_resolution(image.size, self.candidate_resolutions)\n                        images_crop_raw, crop_ratio = dynamic_preprocess(image)\n                    else:\n                        # best_width, best_height = self.image_size, self.image_size\n                        crop_ratio = [1, 1]\n                \n                \"\"\"process the global view\"\"\"\n                # image = image.resize((base_size, base_size))\n                global_view = ImageOps.pad(image, (base_size, base_size),\n                                        color=tuple(int(x * 255) for x in image_transform.mean))\n                \n                if base_size == 1024:\n                    valid_img_tokens += int(256 * ratio)\n                elif base_size == 1280:\n                    valid_img_tokens += int(400 * ratio)\n                \n                images_list.append(image_transform(global_view).to(torch.bfloat16))\n\n                # global_view_tensor = image_transform(global_view).to(torch.bfloat16)\n\n                width_crop_num, height_crop_num = crop_ratio\n\n                images_spatial_crop.append([width_crop_num, height_crop_num])\n                \n                \n                if width_crop_num > 1 or height_crop_num > 1:\n                    \"\"\"process the local views\"\"\"\n                    \n                    for i in range(len(images_crop_raw)):\n                        images_crop_list.append(image_transform(images_crop_raw[i]).to(torch.bfloat16))\n                \n                if image_size == 640:\n                    valid_img_tokens += len(images_crop_list) * 100\n\n                num_queries = math.ceil((image_size // patch_size) / downsample_ratio)\n                num_queries_base = math.ceil((base_size // patch_size) / downsample_ratio)\n                tokenized_image = ([image_token_id] * num_queries_base + [image_token_id]) * num_queries_base\n                tokenized_image += [image_token_id]\n                if width_crop_num > 1 or height_crop_num > 1:\n                    tokenized_image += ([image_token_id] * (num_queries * width_crop_num) + [image_token_id]) * (\n                                num_queries * height_crop_num)\n                tokenized_str += tokenized_image\n                images_seq_mask += [True] * len(tokenized_image)\n                # num_image_tokens.append(len(tokenized_image))\n\n            else:\n                # best_width, best_height = self.image_size, self.image_size\n                # print(image.size, (best_width, best_height)) # check the select_best_resolutions func\n\n                \"\"\"process the global view\"\"\"\n                if image_size <= 640:\n                    print('directly resize')\n                    image = image.resize((image_size, image_size))\n                # else:\n                global_view = ImageOps.pad(image, (image_size, image_size),\n                                        color=tuple(int(x * 255) for x in image_transform.mean))\n                images_list.append(image_transform(global_view).to(torch.bfloat16))\n\n                if base_size == 1024:\n                    valid_img_tokens += int(256 * ratio)\n                elif base_size == 1280:\n                    valid_img_tokens += int(400 * ratio)\n                elif base_size == 640:\n                    valid_img_tokens += int(100 * 1)\n                elif base_size == 512:\n                    valid_img_tokens += int(64 * 1)\n\n                width_crop_num, height_crop_num = 1, 1\n\n                images_spatial_crop.append([width_crop_num, height_crop_num])\n\n\n                \"\"\"add image tokens\"\"\"\n                num_queries = math.ceil((image_size // patch_size) / downsample_ratio)\n\n                tokenized_image = ([image_token_id] * num_queries + [image_token_id]) * num_queries\n                tokenized_image += [image_token_id]\n                # tokenized_image += ([self.image_token_id] * (num_queries * width_crop_num) + [self.image_token_id]) * (\n                #             num_queries * height_crop_num)\n                tokenized_str += tokenized_image\n                images_seq_mask += [True] * len(tokenized_image)\n                # num_image_tokens.append(len(tokenized_image))\n        \n\n            tokenized_sep = text_encode(tokenizer, text_splits[-1], bos=False, eos=False)\n            tokenized_str += tokenized_sep\n            images_seq_mask += [False] * len(tokenized_sep)\n\n            \"\"\"add the bos tokens\"\"\"\n            bos_id = 0\n            tokenized_str = [bos_id] + tokenized_str \n            images_seq_mask = [False] + images_seq_mask\n            input_ids = torch.LongTensor(tokenized_str)\n            images_seq_mask = torch.tensor(images_seq_mask, dtype=torch.bool)\n\n            if len(images_list) == 0:\n                images_ori = torch.zeros((1, 3, image_size, image_size))\n                images_spatial_crop = torch.zeros((1, 2), dtype=torch.long)\n                images_crop = torch.zeros((1, 3, base_size, base_size))\n\n            else:\n                images_ori = torch.stack(images_list, dim=0)\n                images_spatial_crop = torch.tensor(images_spatial_crop, dtype=torch.long)\n                if images_crop_list:\n                    images_crop = torch.stack(images_crop_list, dim=0)\n                else:\n                    images_crop = torch.zeros((1, 3, base_size, base_size))\n\n\n\n            if not eval_mode:\n                streamer = NoEOSTextStreamer(tokenizer, skip_prompt=True, skip_special_tokens=False)\n                with torch.autocast(\"cuda\", dtype=torch.bfloat16):\n                    with torch.no_grad():\n                        output_ids = model.generate(\n                            input_ids.unsqueeze(0).cuda(),\n                            images=[(images_crop.cuda(), images_ori.cuda())],\n                            images_seq_mask = images_seq_mask.unsqueeze(0).cuda(),\n                            images_spatial_crop = images_spatial_crop,\n                            temperature=0.0,\n                            eos_token_id=tokenizer.eos_token_id,\n                            streamer=streamer,\n                            max_new_tokens=8192,\n                            no_repeat_ngram_size = 20,\n                            use_cache = True\n                            )\n\n            else:\n                with torch.autocast(\"cuda\", dtype=torch.bfloat16):\n                    with torch.no_grad():\n                        output_ids = model.generate(\n                            input_ids.unsqueeze(0).cuda(),\n                            images=[(images_crop.cuda(), images_ori.cuda())],\n                            images_seq_mask = images_seq_mask.unsqueeze(0).cuda(),\n                            images_spatial_crop = images_spatial_crop,\n                            temperature=0.0,\n                            eos_token_id=tokenizer.eos_token_id,\n                            max_new_tokens=8192,\n                            no_repeat_ngram_size = 35,\n                            use_cache = True\n                            )\n                    \n\n            if '<image>' in conversation[0]['content'] and eval_mode:\n                    outputs = tokenizer.decode(output_ids[0, input_ids.unsqueeze(0).cuda().shape[1]:])\n                    stop_str = '<｜end▁of▁sentence｜>'\n                    if outputs.endswith(stop_str):\n                        outputs = outputs[:-len(stop_str)]\n                    # re_match\n                    outputs = outputs.strip()\n\n                    return outputs\n            \n            if '<image>' in conversation[0]['content'] and test_compress:\n                outputs = tokenizer.decode(output_ids[0, input_ids.unsqueeze(0).cuda().shape[1]:])\n                pure_texts_outputs_token_length = len(text_encode(tokenizer, outputs, bos=False, eos=False))\n                print('='*50)\n                print('image size: ', (w, h))\n                print('valid image tokens: ', int(valid_img_tokens))\n                print('output texts tokens (valid): ', pure_texts_outputs_token_length)\n                print('compression ratio: ', round(pure_texts_outputs_token_length/valid_img_tokens, 2))\n                print('='*50)\n\n\n            if '<image>' in conversation[0]['content'] and save_results:\n                outputs = tokenizer.decode(output_ids[0, input_ids.unsqueeze(0).cuda().shape[1]:])\n                stop_str = '<｜end▁of▁sentence｜>'\n\n                print('='*15 + 'save results:' + '='*15)\n                \n                # # # # conv.messages[-1][-1] = outputs\n                if outputs.endswith(stop_str):\n                    outputs = outputs[:-len(stop_str)]\n                outputs = outputs.strip()\n\n                matches_ref, matches_images, mathes_other = re_match(outputs)\n                # print(matches_ref)\n                result = draw_bounding_boxes(image_draw, matches_ref, output_path)\n\n\n\n                for idx, a_match_image in enumerate(tqdm(matches_images, desc=\"image\")):\n                    outputs = outputs.replace(a_match_image, '![](images/' + str(idx) + '.jpg)\\n')\n                \n                for idx, a_match_other in enumerate(tqdm(mathes_other, desc=\"other\")):\n                    outputs = outputs.replace(a_match_other, '').replace('\\\\coloneqq', ':=').replace('\\\\eqqcolon', '=:')\n\n\n                # if 'structural formula' in conversation[0]['content']:\n                #     outputs = '<smiles>' + outputs + '</smiles>'\n                with open(f'{output_path}/result.mmd', 'w', encoding = 'utf-8') as afile:\n                    afile.write(outputs)\n\n                if 'line_type' in outputs:\n                    import matplotlib.pyplot as plt\n                    lines = eval(outputs)['Line']['line']\n\n                    line_type = eval(outputs)['Line']['line_type']\n                    # print(lines)\n\n                    endpoints = eval(outputs)['Line']['line_endpoint']\n\n                    fig, ax = plt.subplots(figsize=(3,3), dpi=200)\n                    ax.set_xlim(-15, 15)\n                    ax.set_ylim(-15, 15)\n\n                    for idx, line in enumerate(lines):\n                        try:\n                            p0 = eval(line.split(' -- ')[0])\n                            p1 = eval(line.split(' -- ')[-1])\n\n                            if line_type[idx] == '--':\n                                ax.plot([p0[0], p1[0]], [p0[1], p1[1]], linewidth=0.8, color='k')\n                            else:\n                                ax.plot([p0[0], p1[0]], [p0[1], p1[1]], linewidth = 0.8, color = 'k')\n\n                            ax.scatter(p0[0], p0[1], s=5, color = 'k')\n                            ax.scatter(p1[0], p1[1], s=5, color = 'k')\n                        except:\n                            pass\n\n                    for endpoint in endpoints:\n\n                        label = endpoint.split(': ')[0]\n                        (x, y) = eval(endpoint.split(': ')[1])\n                        ax.annotate(label, (x, y), xytext=(1, 1), textcoords='offset points', \n                                    fontsize=5, fontweight='light')\n                    \n\n                    plt.savefig(f'{output_path}/geo.jpg')\n                    plt.close()\n\n                result.save(f\"{output_path}/result_with_boxes.jpg\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-03T14:56:59.884737Z","iopub.execute_input":"2025-11-03T14:56:59.885537Z","iopub.status.idle":"2025-11-03T14:56:59.927504Z","shell.execute_reply.started":"2025-11-03T14:56:59.885513Z","shell.execute_reply":"2025-11-03T14:56:59.926669Z"}},"outputs":[],"execution_count":32},{"cell_type":"code","source":"from datasets import load_dataset\n\n# Login using e.g. `huggingface-cli login` to access this dataset\nds = load_dataset(\"NAMAA-Space/QariOCR-v0.3-markdown-mixed-dataset\", split=\"test\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-03T14:50:59.180390Z","iopub.execute_input":"2025-11-03T14:50:59.180669Z","iopub.status.idle":"2025-11-03T14:51:04.362420Z","shell.execute_reply.started":"2025-11-03T14:50:59.180650Z","shell.execute_reply":"2025-11-03T14:51:04.361860Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"README.md: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7eed228873ba4917a4139ad5e54a3cb8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"data/train-00000-of-00001.parquet:   0%|          | 0.00/25.2M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3574e50c37d5473cb3d5007316d599ef"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"data/validation-00000-of-00001.parquet:   0%|          | 0.00/3.08M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f8e69ffefeb9489ead77edaa3bc09c09"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"data/test-00000-of-00001.parquet:   0%|          | 0.00/3.18M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c4026f074eb846938318e5490ef37cda"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/29563 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2943252b6961488eb891cc4107915f7e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split:   0%|          | 0/3696 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"187ed46bdbfd4a1085638bcf5a608f18"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/3695 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4a220fbdc809490f835a612d19204586"}},"metadata":{}}],"execution_count":16},{"cell_type":"code","source":"image_file = ds['image'][0]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-03T14:52:13.332007Z","iopub.execute_input":"2025-11-03T14:52:13.332884Z","iopub.status.idle":"2025-11-03T14:52:13.338697Z","shell.execute_reply.started":"2025-11-03T14:52:13.332851Z","shell.execute_reply":"2025-11-03T14:52:13.338115Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"from torchvision import transforms\nfrom torchvision.transforms.functional import InterpolationMode","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-03T14:55:52.051924Z","iopub.execute_input":"2025-11-03T14:55:52.052206Z","iopub.status.idle":"2025-11-03T14:55:52.056134Z","shell.execute_reply.started":"2025-11-03T14:55:52.052188Z","shell.execute_reply":"2025-11-03T14:55:52.055153Z"}},"outputs":[],"execution_count":27},{"cell_type":"code","source":"import requests\nfrom io import BytesIO\n\nresponse = requests.get(image_file)\npil_image = Image.open(BytesIO(response.content)).convert(\"RGB\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-03T14:52:38.579842Z","iopub.execute_input":"2025-11-03T14:52:38.580478Z","iopub.status.idle":"2025-11-03T14:52:38.866208Z","shell.execute_reply.started":"2025-11-03T14:52:38.580455Z","shell.execute_reply":"2025-11-03T14:52:38.865586Z"}},"outputs":[],"execution_count":21},{"cell_type":"code","source":"a = infer_with_image_object(model, tokenizer, pil_image, output_path=\"deepseek_eval\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-03T14:57:03.850536Z","iopub.execute_input":"2025-11-03T14:57:03.850834Z","iopub.status.idle":"2025-11-03T14:57:13.594234Z","shell.execute_reply.started":"2025-11-03T14:57:03.850811Z","shell.execute_reply":"2025-11-03T14:57:13.593606Z"}},"outputs":[{"name":"stderr","text":"The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"=====================\nBASE:  torch.Size([1, 256, 1280])\nPATCHES:  torch.Size([6, 100, 1280])\n=====================\n","output_type":"stream"}],"execution_count":33},{"cell_type":"code","source":"a","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-03T14:57:15.384565Z","iopub.execute_input":"2025-11-03T14:57:15.384832Z","iopub.status.idle":"2025-11-03T14:57:15.390276Z","shell.execute_reply.started":"2025-11-03T14:57:15.384814Z","shell.execute_reply":"2025-11-03T14:57:15.389504Z"}},"outputs":[{"execution_count":34,"output_type":"execute_result","data":{"text/plain":"'<|ref|>text<|/ref|><|det|>[[160, 106, 875, 343]]<|/det|>\\nتلقت المغنية فاطمة الزهراء العروسي، رسالة من الملك محمد السادس، بعد تقديمها لأغنية «على سلامة سيدنا» إهداء للملك، بمناسبة شفائه بعد إجرائه لعملية جراحية، والتي تكالت بالنجاح ونشرت العروسي نص الرسالة على صفحاتها الرسمية على مواقع التواصل، حيث وجهت الشكر للملك على الرسالة مؤكدة على أنها تركت لديها أثرا عميقا وبرهجة وسرور، وأن الإلتفافة ستكون حافزا لديها لتقديم المزيد من العطاء والإجتهاد \\n\\n<|ref|>text<|/ref|><|det|>[[469, 373, 864, 421]]<|/det|>\\n© حقوق النشر: DR'"},"metadata":{}}],"execution_count":34},{"cell_type":"code","source":"import re\n\ndef extract_clean_text(input_str):\n    # Remove all <|...|> tags and whitespace-only lines\n    # Keep only lines that contain actual text (non-tag content)\n    lines = input_str.strip().split('\\n')\n    clean_lines = []\n    for line in lines:\n        # Strip leading/trailing whitespace\n        if line.strip().startswith(\"<|\"):\n          continue\n        stripped = line.strip()\n        # Skip empty lines\n        if not stripped:\n            continue\n        # Remove all <|...|> style tags\n        cleaned = re.sub(r'<\\|[^|]*\\|>', '', stripped)\n        # If after removing tags, there's still meaningful text, keep it\n        if cleaned.strip():\n            clean_lines.append(cleaned.strip())\n\n\n    return re.sub('<｜end▁of▁sentence｜>', '', '\\n'.join(clean_lines))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-03T14:57:30.800407Z","iopub.execute_input":"2025-11-03T14:57:30.800707Z","iopub.status.idle":"2025-11-03T14:57:30.805890Z","shell.execute_reply.started":"2025-11-03T14:57:30.800686Z","shell.execute_reply":"2025-11-03T14:57:30.805217Z"}},"outputs":[],"execution_count":35},{"cell_type":"code","source":"extract_clean_text(a)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-03T14:57:37.917444Z","iopub.execute_input":"2025-11-03T14:57:37.918190Z","iopub.status.idle":"2025-11-03T14:57:37.923249Z","shell.execute_reply.started":"2025-11-03T14:57:37.918159Z","shell.execute_reply":"2025-11-03T14:57:37.922625Z"}},"outputs":[{"execution_count":36,"output_type":"execute_result","data":{"text/plain":"'تلقت المغنية فاطمة الزهراء العروسي، رسالة من الملك محمد السادس، بعد تقديمها لأغنية «على سلامة سيدنا» إهداء للملك، بمناسبة شفائه بعد إجرائه لعملية جراحية، والتي تكالت بالنجاح ونشرت العروسي نص الرسالة على صفحاتها الرسمية على مواقع التواصل، حيث وجهت الشكر للملك على الرسالة مؤكدة على أنها تركت لديها أثرا عميقا وبرهجة وسرور، وأن الإلتفافة ستكون حافزا لديها لتقديم المزيد من العطاء والإجتهاد\\n© حقوق النشر: DR'"},"metadata":{}}],"execution_count":36},{"cell_type":"code","source":"re.sub(r'<[^>]+>','',ds['text'][0])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-03T15:00:03.721757Z","iopub.execute_input":"2025-11-03T15:00:03.722056Z","iopub.status.idle":"2025-11-03T15:00:03.728951Z","shell.execute_reply.started":"2025-11-03T15:00:03.722037Z","shell.execute_reply":"2025-11-03T15:00:03.728018Z"}},"outputs":[{"execution_count":42,"output_type":"execute_result","data":{"text/plain":"'تلقت المغنية فاطمة الزهراء العروسي، رسالة من الملك محمد السادس، بعد تقديمها لأغنية «على سلامة سيدنا » إهداء للملك، بمناسبة شفائه بعد إجرائه لعميلة جراحية، والتي تكللت بالنجاح\\nونشرت العروسي نص الرسالة على صفحاتها الرسمية على مواقع التواصل، حيث وجهت الشكر للملك على الرسالة مؤكدة على أنها تركت لديها أثرا عميقا وبهجة وسرور، وأن الإلتفافة ستكون حافزا لديها لتقديم المزيد من العطاء والإجتهاد\\n© حقوق النشر : DR'"},"metadata":{}}],"execution_count":42},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}